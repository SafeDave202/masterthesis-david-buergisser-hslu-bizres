{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\daveb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\daveb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\daveb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\daveb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import io\n",
    "import itertools\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "import contractions\n",
    "import gensim.downloader as api\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import spacy\n",
    "import torch\n",
    "import torchvision\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import TreebankWordTokenizer, sent_tokenize, word_tokenize\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from pdfminer3.converter import PDFPageAggregator, TextConverter\n",
    "from pdfminer3.layout import LAParams, LTTextBox\n",
    "from pdfminer3.pdfinterp import PDFPageInterpreter, PDFResourceManager\n",
    "from pdfminer3.pdfpage import PDFPage\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "from spacy.lang.en import English\n",
    "from transformers import pipeline\n",
    "\n",
    "sys.path.append(r\"..\")\n",
    "\n",
    "from nlp_functions import (classifier, remove_colons, remove_digits, remove_n,\n",
    "                           remove_redundant_whitespaces,\n",
    "                           remove_strange_characters, remove_stripes,\n",
    "                           text_loader)\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_loader(company_name): \n",
    "    \"\"\"Nimmt PDF-File Namen entgegen. Gibt Text als String, PDF-Namen, Pfad inkl. PDF-Namen sowie Pfad ohne PDF-Namen zurück\"\"\"\n",
    "    company_name = company_name[:-4]\n",
    "    source = r'..\\Data\\Nachhaltigkeitsberichte\\Alle'\n",
    "    path = rf\"{source}\\{company_name}.pdf\"\n",
    "\n",
    "    text = text_loader(path)   \n",
    "\n",
    "    return text, company_name, path, source \n",
    "\n",
    "def topic_modeler(text):\n",
    "    \"\"\"Errechnet die Klassifizierungs-Wahrscheinlichkeiten für den Text für die vorgegebenen Label aus\"\"\"\n",
    "\n",
    "    classifier_pipeline = pipeline(\n",
    "        \"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "    input_sequence = text\n",
    "    label_candidate = ['sustainability', 'human rights',\n",
    "                       'fraud', 'social issues', 'labour law']\n",
    "    x = classifier_pipeline(input_sequence, label_candidate)\n",
    "    result_label = x[\"labels\"]\n",
    "    result_score = x[\"scores\"]\n",
    "    tuple_for_df = list(zip(result_label, result_score))\n",
    "    df_topic_modeling_score = pd.DataFrame(\n",
    "        tuple_for_df, columns=[\"Label\", \"Score\"])\n",
    "    return df_topic_modeling_score\n",
    "\n",
    "def preprocessing_text(text):\n",
    "    \"\"\"Wendet verschiedene Standard-Preprocessings auf den Text an.\"\"\"\n",
    "\n",
    "\n",
    "    text = remove_strange_characters(text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "\n",
    "    text = remove_n(text)\n",
    "\n",
    "    text = remove_colons(text)\n",
    "\n",
    "    text = text.replace(r\" .\",\".\")\n",
    "    text = text.replace(r\"..\",\".\")\n",
    "    text = text.replace(r\"...\",\".\")\n",
    "\n",
    "    text = remove_stripes(text)\n",
    "\n",
    "    text = remove_redundant_whitespaces(text)\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TI-IDF Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_words(text):\n",
    "    \"\"\"Formt Worte im Text in ihre Lemma um\"\"\"\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = word_tokenize(text)\n",
    "\n",
    "    lemma_list = []\n",
    "\n",
    "    for word in text:\n",
    "        lemma_word = lemmatizer.lemmatize(word)\n",
    "        lemma_list.append(lemma_word)\n",
    "\n",
    "    lemma_text = ' '.join(lemma_list)\n",
    "\n",
    "\n",
    "    return lemma_text\n",
    "\n",
    "\n",
    "\n",
    "def remove_small_tokens(prep_text):\n",
    "    \"\"\"Entfernt Tokens, welche kleiner als Vier sind.\"\"\"\n",
    "    prep_text_before = prep_text\n",
    "    prep_text_before = word_tokenize(prep_text_before)\n",
    "\n",
    "    for word in prep_text_before:\n",
    "        if len(word) <= 3:\n",
    "            prep_text_before.remove(word)\n",
    "    prep_text_list = ' '.join(prep_text_before)\n",
    "\n",
    "\n",
    "    return prep_text_list\n",
    "\n",
    "def get_top_n_words(corpus, n=None):\n",
    "    \"\"\"Errechnet die N-Meistgenannten Worte (BOW)\"\"\"\n",
    "    corpus=[corpus]\n",
    "\n",
    "    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    top_n_words_df = pd.DataFrame(words_freq, columns =['Word', 'Count'])\n",
    "\n",
    "    return top_n_words_df.loc[:(n-1)]\n",
    "\n",
    "\n",
    "def get_tf_idf(text, n=None):\n",
    "    \"\"\"Errechnet den TF-IDF Score für die N-höchsten Scores\"\"\"\n",
    "    text = re.findall(r'(?:\\d[.]|[^.])*(?:[.]|$)', text)\n",
    "    tfIdfVectorizer=TfidfVectorizer(use_idf=True)\n",
    "    tfIdf = tfIdfVectorizer.fit_transform(text)\n",
    "    df = pd.DataFrame(tfIdf[0].T.todense(), index=tfIdfVectorizer.get_feature_names(), columns=[\"TF-IDF\"])\n",
    "    df = df.sort_values('TF-IDF', ascending=False)\n",
    "    return (df[:(n)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_companies = os.listdir(r\"..\\Data\\Nachhaltigkeitsberichte\\Alle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We process now 4f303cec-a12d-480b-accb-7b56f706f60e_axa-ri2020-en-accessible.pdf\n",
      "axa_health_customer SAFED\n",
      "We process now 4f391131-ad12-ab53-7265-5e6c88840627.pdf\n",
      "global_safety_training SAFED\n",
      "We process now 5ZmOsI2P3oe0plCvOThrCySgcDcKCXqj.pdf\n",
      "die_lindt_sprüngli SAFED\n",
      "We process now 9clin.pdf\n",
      "chemical_mitsubishi_employee SAFED\n",
      "We process now 10_BCGB20_SustainabilityReport_E_Web.pdf\n",
      "cid_burckhardt_compression SAFED\n",
      "We process now 668a8a26-d924-21aa-cd75-c12e5296ff2b.pdf\n",
      "bcge_financial_board SAFED\n",
      "We process now 1212.pdf\n",
      "bell_food_group SAFED\n",
      "We process now 1313.pdf\n",
      "bell_food_group SAFED\n",
      "We process now 2012-CSR-report.pdf\n",
      "dsv_employee_target SAFED\n",
      "We process now 2017-Glencore-Sustainability-Report-FINAL-.pdf\n",
      "community_glencore_report SAFED\n",
      "We process now 2019_Straumann_sustainability_report.pdf\n",
      "dental_report_production SAFED\n",
      "We process now 2019-sustainability-report-doc-en.pdf\n",
      "swiss_risk_sustainability SAFED\n",
      "We process now 2020_dnf_-_eng_0.pdf\n",
      "group_autogrill_management SAFED\n",
      "We process now 2020_Sustainability_Report.pdf\n",
      "braun_employee_management SAFED\n",
      "We process now 2020_valora_geschaeftsbericht_de.pdf\n",
      "valora_financial_group SAFED\n",
      "We process now 2020-Annual-Report-7u42lsu22.pdf\n",
      "temenos_financial_year SAFED\n",
      "We process now 2020-responsibility-highlights-report.pdf\n",
      "vifor_pharma_patient SAFED\n",
      "We process now 2020-sustainability-report-doc-en.pdf\n",
      "swiss_sustainability_risk SAFED\n",
      "We process now 2021-03-22_JuliusBaer_CorporateSustainabilityReport2020_EN.pdf\n",
      "2021-03-22_JuliusBaer_CorporateSustainabilityReport2020_EN.pdf didn't work\n",
      "We process now 210928_UBP20Sustainability20Report.pdf\n"
     ]
    }
   ],
   "source": [
    "for company in list_of_companies:\n",
    "    try:\n",
    "        print(f\"We process now {company}\")\n",
    "        \n",
    "        ## BOW / TF-IDF\n",
    "\n",
    "        text_1, company_name, path, source = pdf_loader(company) #\n",
    "        prep_text = preprocessing_text(text_1)\n",
    "        lemma_words = lemmatize_words(prep_text)\n",
    "        text_cleaned = remove_small_tokens(lemma_words)\n",
    "        top_n_words = get_top_n_words(text_cleaned, 20)\n",
    "        top_n_words = list(top_n_words.itertuples(index=False, name=None))\n",
    "        tf_idf = get_tf_idf(text_cleaned, 20)\n",
    "        tf_idf = tf_idf.reset_index()\n",
    "        tf_idf = tf_idf.rename(columns={\"index\": \"Word\"})\n",
    "        idf_list = list(tf_idf.itertuples(index=False, name=None))\n",
    "        data = [(company_name, idf_list, top_n_words)]\n",
    "        df1 = pd.DataFrame(data, columns = [\"Company Name\", \"TF-IDF\", \"Top N Words\"])\n",
    "        company_name_new = f\"{df1['Top N Words'][0][0][0]}_{df1['Top N Words'][0][1][0]}_{df1['Top N Words'][0][2][0]}\"\n",
    "\n",
    "        ## Full Text Topic Modeling\n",
    "\n",
    "        text = pdf_loader(company)\n",
    "        text = preprocessing_text(text)\n",
    "        result_df = topic_modeler(text)\n",
    "        result_df.to_csv(\n",
    "            fr\"..\\Data\\Resultate\\Testfolder\\Zero Shot Learning Fulltext\\{company_name_new}_FullText_TopicModeling.csv\")\n",
    "\n",
    "        print(f\"{company_name_new} SAFED\")\n",
    "    except:\n",
    "        print(f\"{company} didn't work\" )\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fcf02d00a6b3e7f0f947d643ae902f18bdd390f284f55fc20de5769d141d0127"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
