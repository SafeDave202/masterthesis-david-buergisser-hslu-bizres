{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install pdfminer3\n",
    "# !pip3 install contractions\n",
    "# !pip3 install spacy\n",
    "# !pip3 install unicodedata\n",
    "# !pip3 install nltk\n",
    "# !pip3 install numpy\n",
    "# !pip3 install string\n",
    "# !pip3 install pandas\n",
    "# !pip3 install gensim\n",
    "# !pip3 install sklearn\n",
    "# !pip3 install matplotlib\n",
    "# !pip3 install plotly\n",
    "# !pip3 install torch\n",
    "# !pip3 install torchvision\n",
    "# !pip install --upgrade gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install transformers\n",
    "# !pip3 install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\daveb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\daveb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from nlp_functions import text_loader, classifier, remove_strange_characters, remove_n, remove_colons, remove_stripes, remove_redundant_whitespaces, remove_digits\n",
    "import torchvision\n",
    "import torch\n",
    "import plotly.express as px\n",
    "from transformers import pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.manifold import TSNE\n",
    "import gensim.downloader as api\n",
    "from spacy.lang.en import English\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "import shutil\n",
    "import io\n",
    "from pdfminer3.converter import TextConverter\n",
    "from pdfminer3.converter import PDFPageAggregator\n",
    "from pdfminer3.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer3.pdfinterp import PDFResourceManager\n",
    "from pdfminer3.pdfpage import PDFPage\n",
    "from pdfminer3.layout import LAParams, LTTextBox\n",
    "import contractions\n",
    "import string\n",
    "import numpy as np\n",
    "import spacy\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "import collections\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def pdf_loader(company_name):\n",
    "    company_name = company_name[:-4]\n",
    "    source = 'D:\\Google Drive\\Meine Ablage\\HSLU\\Master\\Master Thesis\\Coding Area Bulldozer\\Data\\Sustainability Reports\\To Do'\n",
    "    path = rf\"{source}\\{company_name}.pdf\"\n",
    "\n",
    "    text = text_loader(path)\n",
    "\n",
    "    return text, company_name, path, source\n",
    "\n",
    "\n",
    "def preprocessing_df(text):\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    sentences = []\n",
    "    for s in sent_tokenize(text):\n",
    "        sentences.append(s)\n",
    "\n",
    "    df = pd.DataFrame(sentences, columns=['Sentences'])\n",
    "\n",
    "    df['Alt_Text'] = df['Sentences'].apply(\n",
    "        lambda x: remove_strange_characters(x))\n",
    "    df['Alt_Text'] = df['Alt_Text'].apply(lambda x: remove_n(x))\n",
    "    df['Alt_Text'] = df['Alt_Text'].apply(lambda x: remove_stripes(x))\n",
    "    df['Alt_Text'] = df['Alt_Text'].apply(\n",
    "        lambda x: remove_redundant_whitespaces(x))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def classifier_pipeline(dataframe):\n",
    "    token_ls = dataframe[\"Alt_Text\"].tolist()\n",
    "    threshold = 0.5\n",
    "    classification_ls = []\n",
    "    short_ls = token_ls[:]\n",
    "    print(len(short_ls))\n",
    "    for i in short_ls:  # removing undedected empty vectors\n",
    "        if len(i) < 1:\n",
    "            short_ls.remove(i)\n",
    "    print(len(short_ls))\n",
    "\n",
    "    for i in short_ls:\n",
    "        # token_str = ' '.join(i)\n",
    "        x = classifier(i)\n",
    "        if x.get(\"scores\")[0] > threshold:\n",
    "            classification_ls.append(x.get(\"labels\")[0])\n",
    "        else:\n",
    "            classification_ls.append(\"other\")\n",
    "        classification_ls\n",
    "\n",
    "    return classification_ls, short_ls\n",
    "\n",
    "\n",
    "def rebuilding(classification_ls, short_ls):\n",
    "    sustainability_text = []\n",
    "    human_rights_text = []\n",
    "    fraud_text = []\n",
    "    social_issue_text = []\n",
    "    employee_affairs_text = []\n",
    "    other_text = []\n",
    "\n",
    "    for (text, label) in zip(classification_ls, short_ls):\n",
    "        if label == \"sustainability\":\n",
    "            sustainability_text.append(text)\n",
    "        elif label == \"human rights\":\n",
    "            human_rights_text.append(text)\n",
    "        elif label == \"fraud\":\n",
    "            fraud_text.append(text)\n",
    "        elif label == \"social issues\":\n",
    "            social_issue_text.append(text)\n",
    "        elif label == \"labour law\":\n",
    "            employee_affairs_text.append(text)\n",
    "        elif label == \"other\":\n",
    "            other_text.append(text)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    nested_label_list = [sustainability_text, human_rights_text,\n",
    "                         fraud_text, social_issue_text, employee_affairs_text, other_text]\n",
    "\n",
    "    return nested_label_list\n",
    "\n",
    "\n",
    "def list_to_string_join(input_list):\n",
    "    string = []\n",
    "    for item in input_list:\n",
    "        string.append(item)\n",
    "    joined_string = ' '.join(string)\n",
    "\n",
    "    return joined_string\n",
    "\n",
    "\n",
    "def labeling_nested_list(input_list):\n",
    "    nested_label_list = input_list\n",
    "    label_string = []\n",
    "    for label in nested_label_list:\n",
    "        label_string.append(list_to_string_join(label))\n",
    "\n",
    "    label_candidate = ['sustainability', 'human rights',\n",
    "                       'fraud', 'social issues', 'labour law', \"others\"]\n",
    "    tuples_prep = list(zip(label_candidate, label_string))\n",
    "    report_df = pd.DataFrame(tuples_prep, columns=[\"Label\", \"Text\"])\n",
    "    return report_df\n",
    "\n",
    "\n",
    "def similarity_function(text_1, text_2):\n",
    "    sentences = [text_1, text_2]\n",
    "\n",
    "    # !pip install sentence_transformers\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "    model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "    sentence_embeddings = model.encode(sentences)\n",
    "    sentence_embeddings.shape\n",
    "    return cosine_similarity(\n",
    "        [sentence_embeddings[0]],\n",
    "        sentence_embeddings[1:]\n",
    "    )\n",
    "\n",
    "\n",
    "def scoring(report_df):\n",
    "    wiki_text = pd.read_csv(\n",
    "        r\"D:\\Google Drive\\Meine Ablage\\HSLU\\Master\\Master Thesis\\Coding Area Bulldozer\\Data\\wiki_artikel_summary.csv\")\n",
    "    result_cosine_similarity = []\n",
    "    wiki_text_list = wiki_text[\"Text\"].to_list()\n",
    "    wiki_labels_list = wiki_text[\"Label\"].to_list()\n",
    "    report_text_list = report_df[\"Text\"].to_list()\n",
    "\n",
    "    for w_text, r_text in zip(wiki_text_list, report_text_list):\n",
    "        pre_result = (similarity_function(w_text, r_text))\n",
    "        result_cosine_similarity.append(pre_result[0][0])\n",
    "\n",
    "    data_tuples = list(zip(wiki_labels_list, result_cosine_similarity))\n",
    "    result_df = pd.DataFrame(data_tuples, columns=['Label', 'Score'])\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def preprocessing_text(text):\n",
    "    # mapping = str.maketrans('', '', string.digits)\n",
    "    # text = text.translate(mapping)\n",
    "\n",
    "    text = remove_strange_characters(text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    text = remove_n(text)\n",
    "\n",
    "    text = remove_colons(text)\n",
    "\n",
    "    text = text.replace(r\" .\", \".\")\n",
    "    text = text.replace(r\"..\", \".\")\n",
    "    text = text.replace(r\"...\", \".\")\n",
    "\n",
    "    text = remove_stripes(text)\n",
    "\n",
    "    text = remove_redundant_whitespaces(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def topic_modeler(text):\n",
    "\n",
    "    classifier_pipeline = pipeline(\n",
    "        \"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "    input_sequence = text\n",
    "    label_candidate = ['sustainability', 'human rights',\n",
    "                       'fraud', 'social issues', 'labour law']\n",
    "    x = classifier_pipeline(input_sequence, label_candidate)\n",
    "    result_label = x[\"labels\"]\n",
    "    result_score = x[\"scores\"]\n",
    "    tuple_for_df = list(zip(result_label, result_score))\n",
    "    df_topic_modeling_score = pd.DataFrame(\n",
    "        tuple_for_df, columns=[\"Label\", \"Score\"])\n",
    "    return df_topic_modeling_score\n",
    "\n",
    "\n",
    "def similarity_function(text_1, text_2):\n",
    "    sentences = [text_1, text_2]\n",
    "\n",
    "    # !pip install sentence_transformers\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "    model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "    sentence_embeddings = model.encode(sentences)\n",
    "    sentence_embeddings.shape\n",
    "    return cosine_similarity(\n",
    "        [sentence_embeddings[0]],\n",
    "        sentence_embeddings[1:]\n",
    "    )\n",
    "\n",
    "\n",
    "def results_for_each_topic(text):\n",
    "    wiki_text = pd.read_csv(\n",
    "        r\"D:\\Google Drive\\Meine Ablage\\HSLU\\Master\\Master Thesis\\Coding Area Bulldozer\\Data\\wiki_artikel_summary.csv\")\n",
    "\n",
    "    result_cosine_similarity = []\n",
    "    labels = wiki_text[\"Label\"].to_list()\n",
    "    for label_text in wiki_text[\"Text\"]:\n",
    "        pre_result = (similarity_function(label_text, text))\n",
    "        result_cosine_similarity.append(pre_result[0][0])\n",
    "\n",
    "    data_tuples = list(zip(labels, result_cosine_similarity))\n",
    "    result_df = pd.DataFrame(data_tuples, columns=['Label', 'Score'])\n",
    "    return result_df\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TI-IDF Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\daveb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\daveb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "def preprocessing(text):\n",
    "    # mapping = str.maketrans('', '', string.digits)\n",
    "    # text = text.translate(mapping)\n",
    "\n",
    "    text = remove_strange_characters(text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "\n",
    "    text = remove_n(text)\n",
    "\n",
    "    text = remove_colons(text)\n",
    "\n",
    "    text = text.replace(r\" .\",\".\")\n",
    "    text = text.replace(r\"..\",\".\")\n",
    "    text = text.replace(r\"...\",\".\")\n",
    "\n",
    "    text = remove_stripes(text)\n",
    "\n",
    "    text = remove_redundant_whitespaces(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def lemmatize_words(text):\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = word_tokenize(text)\n",
    "\n",
    "    lemma_list = []\n",
    "\n",
    "    for word in text:\n",
    "        lemma_word = lemmatizer.lemmatize(word)\n",
    "        lemma_list.append(lemma_word)\n",
    "\n",
    "    lemma_text = ' '.join(lemma_list)\n",
    "\n",
    "\n",
    "    return lemma_text\n",
    "\n",
    "\n",
    "\n",
    "def remove_small_tokens(prep_text):\n",
    "    prep_text_before = prep_text\n",
    "    # print(f\"Text before cleaning:  {len(prep_text_before)}\")\n",
    "    # print(type(prep_text_before))\n",
    "    # prep_text_before = [prep_text_before]\n",
    "    prep_text_before = word_tokenize(prep_text_before)\n",
    "    # print(prep_text_before)\n",
    "\n",
    "    for word in prep_text_before:\n",
    "        if len(word) <= 3:\n",
    "            prep_text_before.remove(word)\n",
    "    # print(f\"Text after cleaning:  {len(prep_text_before)}\")\n",
    "    prep_text_list = ' '.join(prep_text_before)\n",
    "    # print(type(prep_text_list))\n",
    "\n",
    "\n",
    "    return prep_text_list\n",
    "\n",
    "# BOW\n",
    "\n",
    "def get_top_n_words(corpus, n=None):\n",
    "    corpus=[corpus]\n",
    "\n",
    "    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    top_n_words_df = pd.DataFrame(words_freq, columns =['Word', 'Count'])\n",
    "\n",
    "    return top_n_words_df.loc[:(n-1)]\n",
    "\n",
    "\n",
    "def get_tf_idf(text, n=None):\n",
    "    text = re.findall(r'(?:\\d[.]|[^.])*(?:[.]|$)', text)\n",
    "    tfIdfVectorizer=TfidfVectorizer(use_idf=True)\n",
    "    tfIdf = tfIdfVectorizer.fit_transform(text)\n",
    "    df = pd.DataFrame(tfIdf[0].T.todense(), index=tfIdfVectorizer.get_feature_names(), columns=[\"TF-IDF\"])\n",
    "    df = df.sort_values('TF-IDF', ascending=False)\n",
    "    return (df[:(n)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_of_companies = os.listdir(\n",
    "#     r\"D:\\Google Drive\\Meine Ablage\\HSLU\\Master\\Master Thesis\\Coding Area\\Data\\Sustainability Reports\\To Do\")\n",
    "\n",
    "# for company in list_of_companies:\n",
    "#     company_name = company\n",
    "#     text = pdf_loader(company)\n",
    "#     text = preprocessing_text(text)\n",
    "#     result_df = topic_modeler(text)\n",
    "#     result_df.to_csv(\n",
    "#         fr\"D:\\Google Drive\\Meine Ablage\\HSLU\\Master\\Master Thesis\\Coding Area\\Data\\Results\\{company_name[:-4]}_FullText_TopicModeling.csv\")\n",
    "\n",
    "# for company in list_of_companies:\n",
    "#     text_1, company_name, path, source = pdf_loader(company)\n",
    "#     text_2 = preprocessing_text(text_1)\n",
    "#     result_df = results_for_each_topic(text_2)\n",
    "#     result_df.to_csv(\n",
    "#         fr\"D:\\Google Drive\\Meine Ablage\\HSLU\\Master\\Master Thesis\\Coding Area\\Data\\Results\\{company_name}_FullText_Scoring.csv\")\n",
    "\n",
    "# frames = []\n",
    "\n",
    "# for company in list_of_companies:\n",
    "#     text_1, company_name, path, source = pdf_loader(company)\n",
    "#     prep_text = preprocessing(text_1)\n",
    "#     lemma_words = lemmatize_words(prep_text)\n",
    "#     text_cleaned = remove_small_tokens(lemma_words)\n",
    "#     top_n_words = get_top_n_words(text_cleaned, 20)\n",
    "#     top_n_words = list(top_n_words.itertuples(index=False, name=None))\n",
    "#     top_n_words\n",
    "#     tf_idf = get_tf_idf(text_cleaned, 20)\n",
    "#     tf_idf = tf_idf.reset_index()\n",
    "#     tf_idf = tf_idf.rename(columns={\"index\": \"Word\"})\n",
    "#     idf_list = list(tf_idf.itertuples(index=False, name=None))\n",
    "#     idf_list\n",
    "#     data = [(company_name, idf_list, top_n_words)]\n",
    "#     df1 = pd.DataFrame(data, columns = [\"Company Name\", \"TF-IDF\", \"Top N Words\"])\n",
    "#     frames.append(df1)\n",
    "#     df_final = pd.concat(frames)\n",
    "#     df_final.to_csv(fr\"D:\\Google Drive\\Meine Ablage\\HSLU\\Master\\Master Thesis\\Coding Area\\Data\\Results\\{company_name}_bow_tf_ifd.csv\")\n",
    "\n",
    "# for company in list_of_companies:\n",
    "#     text_1, company_name, path, source = pdf_loader(company)\n",
    "#     df_1 = preprocessing_df(text_1)\n",
    "#     classification_ls, short_ls = classifier_pipeline(df_1)\n",
    "#     nested_label_list = rebuilding(classification_ls, short_ls)\n",
    "#     report_df = labeling_nested_list(nested_label_list)\n",
    "#     results_df = scoring(report_df)\n",
    "#     results_df.to_csv(\n",
    "#         fr\"D:\\Google Drive\\Meine Ablage\\HSLU\\Master\\Master Thesis\\Coding Area\\Data\\Results\\{company_name}_Paragraphed_Cosine_Scoring.csv\")\n",
    "#     shutil.move(\n",
    "#         fr\"{path}\", fr\"D:\\Google Drive\\Meine Ablage\\HSLU\\Master\\Master Thesis\\Coding Area\\Data\\Sustainability Reports\\Already Done\\{company_name}.pdf\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2019-sustainability-report-doc-en.pdf',\n",
       " '2020-Annual-Report-7u42lsu22.pdf',\n",
       " '2020-responsibility-highlights-report.pdf',\n",
       " '2020-sustainability-report-doc-en.pdf',\n",
       " '20200625_man-es_pr_cr-report_2020_en.pdf',\n",
       " '2020_dnf_-_eng_0.pdf',\n",
       " '2020_Sustainability_Report.pdf',\n",
       " '2020_valora_geschaeftsbericht_de.pdf',\n",
       " '2021-03-22_JuliusBaer_CorporateSustainabilityReport2020_EN.pdf',\n",
       " '210928_UBP20Sustainability20Report.pdf',\n",
       " '2641133_DOWNLOAD.pdf',\n",
       " '5df03cc1-f2ec-65ed-9c3d-da3c343395a5.pdf',\n",
       " '5ZmOsI2P3oe0plCvOThrCySgcDcKCXqj.pdf',\n",
       " '668a8a26-d924-21aa-cd75-c12e5296ff2b.pdf',\n",
       " '9clin.pdf',\n",
       " 'Allianz_Group_Sustainability_Report_2020-web.pdf',\n",
       " 'Allreal_Sustainability_Report_EN_2020.pdf',\n",
       " 'AMAG_Geschaeftsbericht_2021_Magazin_Englisch.pdf',\n",
       " 'annual-and-sustainability-report-2020.pdf',\n",
       " 'apollo-esg-report-v-12.pdf',\n",
       " 'ar21e.pdf',\n",
       " 'avaloq-csr-2020-report.pdf',\n",
       " 'Bucher Sustainability report 2020.pdf',\n",
       " 'c05179523.pdf',\n",
       " 'cargill-aqua-nutrition-sustainability-report.pdf',\n",
       " 'celgene-responsibility.pdf',\n",
       " 'ClariantIntegratedReport2020EN.pdf',\n",
       " 'coca-cola-business-environmental-social-governance-report-2020.pdf',\n",
       " 'COOP_NHB_2011_e_low.pdf',\n",
       " 'Corporate_Sustainability_Report_2019_Web.pdf',\n",
       " 'Coutts20Sustainability20Report.pdf',\n",
       " 'CPN_Fortschrittsbericht_EN_v02.pdf',\n",
       " 'csg-sr-2020-en.pdf',\n",
       " 'CSR20leaflet_A4.pdf',\n",
       " 'CSR_Report_2019_en.pdf',\n",
       " 'CSR_Report_2020.pdf',\n",
       " 'decarbonisation-role-of-TSOs-en.pdf',\n",
       " 'DECATHLON-Nachhaltigkeitsbericht-2019-engl.-2.pdf',\n",
       " 'Dufry20Corporate20Brochure202021.pdf',\n",
       " 'Environmental-Social-and-Governance-TCFD-Report.pdf',\n",
       " 'EN_Post_Nachhaltigkeitsbericht_2019.pdf',\n",
       " 'Feintool_Sustainability_Report_2019_EN_4-8-20.pdf',\n",
       " 'Firmenich-Sustainability-Report-2020.pdf',\n",
       " 'Fussball-WM_ Auch Gastarbeiter sind echte Fussballfans.pdf',\n",
       " 'FY20_Corporate_Sustainability_EN.pdf',\n",
       " 'Geschaeftsbericht_2020_der_Rhaetischen_Bahn.pdf',\n",
       " 'GF_NHB19_Sustainability-at-GF.pdf',\n",
       " 'girsberger-sustainability-report2020.pdf',\n",
       " 'giv-2019-sustainability-gri-index.pdf',\n",
       " 'GKB_2020_Geschaeftsbericht-DE.pdf',\n",
       " 'Glencore_SR202020_Interactive_Final_20optimised.pdf',\n",
       " 'glkb_2020_geschaeftsbericht-kurzform_glarner-kantonalbank-glarus-glarnerland.pdf.pdf',\n",
       " 'glo-annual-report-2020-sustainability-report.pdf',\n",
       " 'google-2021-environmental-report.pdf',\n",
       " 'hiag_gb20-_en.pdf',\n",
       " 'HM-Group-Sustainability-Performance-Report-2020.pdf',\n",
       " 'HOCHDORF_Annual-Report_2020.pdf',\n",
       " 'HSE20Policy.pdf',\n",
       " 'IIC-Sustainability-Report-2020.pdf',\n",
       " 'ikea_sustainability_report_fy20.pdf',\n",
       " 'Implenia_nachhaltigkeit_Pdf_Version_DE.pdf',\n",
       " 'index.pdf',\n",
       " 'IWC_Sustainability_Targets_16.pdf',\n",
       " 'Jahresbericht202020_DE_web.pdf',\n",
       " 'MB_GB2018_EN_Sustainability.pdf',\n",
       " 'Nachhaltigkeitsbericht-2020-DE.pdf',\n",
       " 'nestle-tackling-child-labor-report-2019-en.pdf',\n",
       " 'novartis-environmental-sustainability-occupational-heatlth-safety-data-supplement-2020.pdf',\n",
       " 'ORIOR20Sustainability20Report202020.pdf',\n",
       " 'ProgressReport2019.pdf',\n",
       " 'PSP_GB_2020_en_Nachhaltigkeit.pdf',\n",
       " 'responsibility-report-2020-en.pdf',\n",
       " 'RWyG1q.pdf',\n",
       " 'sap-2019-integrated-report.pdf',\n",
       " 'schindler-corporate-responsibility-report-2020.pdf',\n",
       " 'SD-Payments-to-governments-report-2019.pdf',\n",
       " 'sgs-2020-corporate-sustainability-report.pdf',\n",
       " 'six-corporate-responsibility-report-2020-en.pdf',\n",
       " 'social-impact-report-2020.pdf',\n",
       " 'Sunrise_Annual_Report_2019.pdf',\n",
       " 'sustainability-report-2019_all-in-one-2.pdf',\n",
       " 'sustainability-report-2020-2.pdf',\n",
       " 'sustainability-report-2020-aw-09-spread.pdf',\n",
       " 'sustainability-report-2020.pdf',\n",
       " 'sustainability-report-2021.pdf',\n",
       " 'sustainability-report-fy21-aw-spreads.pdf',\n",
       " 'Sustainability20Key20Figures.pdf.coredownload.pdf',\n",
       " 'SustainabilityReport_EN2020.pdf',\n",
       " 'sustainability_report_2020_web3_0.pdf',\n",
       " 'sustainability_report_en_2019.pdf',\n",
       " 'Swarovski_Sustainability_Report_2021.pdf',\n",
       " 'swatchgroup_sustainability_report_2020 (1).pdf',\n",
       " 'swatchgroup_sustainability_report_2020.pdf',\n",
       " 'SwissLife_SustainabilityReport_2019 (1).pdf',\n",
       " 'SwissLife_SustainabilityReport_2019.pdf',\n",
       " 'Syngenta-ESG-Report-2020.pdf',\n",
       " 'takeda2020sustainabilityreport_en.pdf',\n",
       " 'ubs-sustainability-report-2020.pdf',\n",
       " 'umfrage_digitaler_test_de.pdf',\n",
       " 'Unnamedattachment.pdf',\n",
       " 'Vacheron-Constantin-Due-Diligence-Report.pdf',\n",
       " 'Volg_Geschaeftsbericht-2020.pdf',\n",
       " 'VPBank_Sustainability_report_en.pdf',\n",
       " 'wuerth_nachhaltigkeitsbericht_2019_1.pdf',\n",
       " 'Zegna-responsibility.pdf']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_companies = os.listdir(r\"D:\\Google Drive\\Meine Ablage\\HSLU\\Master\\Master Thesis\\Coding Area Bulldozer\\Data\\Sustainability Reports\\To Do\")\n",
    "list_of_companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We process now 2019-sustainability-report-doc-en.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\daveb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swiss_risk_sustainability SAFED\n",
      "We process now 2020-Annual-Report-7u42lsu22.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\daveb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temenos_financial_year SAFED\n",
      "We process now 2020-responsibility-highlights-report.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\daveb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vifor_pharma_patient SAFED\n",
      "We process now 2020-sustainability-report-doc-en.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\daveb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swiss_sustainability_risk SAFED\n",
      "We process now 20200625_man-es_pr_cr-report_2020_en.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\daveb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "energy_solutions_company SAFED\n",
      "We process now 2020_dnf_-_eng_0.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\daveb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_autogrill_management SAFED\n",
      "We process now 2020_Sustainability_Report.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\daveb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "braun_employee_management SAFED\n",
      "We process now 2020_valora_geschaeftsbericht_de.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\daveb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valora_financial_group SAFED\n",
      "We process now 2021-03-22_JuliusBaer_CorporateSustainabilityReport2020_EN.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\daveb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sustainability_risk_report SAFED\n",
      "We process now 210928_UBP20Sustainability20Report.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\daveb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ubp_sustainability_investment SAFED\n",
      "We process now 2641133_DOWNLOAD.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\daveb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "football_uefa_social SAFED\n",
      "We process now 5df03cc1-f2ec-65ed-9c3d-da3c343395a5.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\daveb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_training_qshe SAFED\n",
      "We process now 5ZmOsI2P3oe0plCvOThrCySgcDcKCXqj.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\daveb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "die_lindt_sprüngli SAFED\n",
      "We process now 668a8a26-d924-21aa-cd75-c12e5296ff2b.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\daveb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bcge_financial_board SAFED\n",
      "We process now 9clin.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\daveb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chemical_mitsubishi_employee SAFED\n",
      "We process now Allianz_Group_Sustainability_Report_2020-web.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\daveb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allianz_Group_Sustainability_Report_2020-web.pdf didn't work\n",
      "We process now Allreal_Sustainability_Report_EN_2020.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\daveb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allreal_report_employee SAFED\n",
      "We process now AMAG_Geschaeftsbericht_2021_Magazin_Englisch.pdf\n"
     ]
    }
   ],
   "source": [
    "\n",
    "list_of_companies = os.listdir(r\"D:\\Google Drive\\Meine Ablage\\HSLU\\Master\\Master Thesis\\Coding Area Bulldozer\\Data\\Sustainability Reports\\To Do\")\n",
    "\n",
    "for company in list_of_companies:\n",
    "    ## BOW / TF-IDF\n",
    "    try:\n",
    "        print(f\"We process now {company}\")\n",
    "\n",
    "        text_1, company_name, path, source = pdf_loader(company)\n",
    "        prep_text = preprocessing(text_1)\n",
    "        lemma_words = lemmatize_words(prep_text)\n",
    "        text_cleaned = remove_small_tokens(lemma_words)\n",
    "        top_n_words = get_top_n_words(text_cleaned, 20)\n",
    "        top_n_words = list(top_n_words.itertuples(index=False, name=None))\n",
    "        top_n_words\n",
    "        tf_idf = get_tf_idf(text_cleaned, 20)\n",
    "        tf_idf = tf_idf.reset_index()\n",
    "        tf_idf = tf_idf.rename(columns={\"index\": \"Word\"})\n",
    "        idf_list = list(tf_idf.itertuples(index=False, name=None))\n",
    "        idf_list\n",
    "        data = [(company_name, idf_list, top_n_words)]\n",
    "        df1 = pd.DataFrame(data, columns = [\"Company Name\", \"TF-IDF\", \"Top N Words\"])\n",
    "        # print(df1)\n",
    "        company_name_new = f\"{df1['Top N Words'][0][0][0]}_{df1['Top N Words'][0][1][0]}_{df1['Top N Words'][0][2][0]}\"\n",
    "        df1.to_csv(fr\"D:\\Google Drive\\Meine Ablage\\HSLU\\Master\\Master Thesis\\Coding Area Bulldozer\\Data\\Results\\Test\\{company_name_new}_bow_tf_idf.csv\")\n",
    "\n",
    "        ## Full Text Topic Modeling\n",
    "\n",
    "        # company_name = company\n",
    "        # text = pdf_loader(company)\n",
    "        # text = preprocessing_text(text)\n",
    "        # result_df = topic_modeler(text)\n",
    "        # result_df.to_csv(\n",
    "        #     fr\"D:\\Google Drive\\Meine Ablage\\HSLU\\Master\\Master Thesis\\Coding Area Bulldozer\\Data\\Results\\Wiki Summary\\{company_name_new}_FullText_TopicModeling.csv\")\n",
    "\n",
    "        ## Full Text Similarity Scoring\n",
    "\n",
    "        text_1, company_name, path, source = pdf_loader(company)\n",
    "        text_2 = preprocessing_text(text_1)\n",
    "        result_df = results_for_each_topic(text_2)\n",
    "        result_df.to_csv(\n",
    "            fr\"D:\\Google Drive\\Meine Ablage\\HSLU\\Master\\Master Thesis\\Coding Area Bulldozer\\Data\\Results\\Test\\{company_name_new}_FullText_Cosine_Scoring_Summary.csv\")\n",
    "\n",
    "\n",
    "        ## Paragraphed Similarity Scoring\n",
    "\n",
    "        # text_1, company_name, path, source = pdf_loader(company)\n",
    "        # df_1 = preprocessing_df(text_1)\n",
    "        # classification_ls, short_ls = classifier_pipeline(df_1)\n",
    "        # nested_label_list = rebuilding(classification_ls, short_ls)\n",
    "        # report_df = labeling_nested_list(nested_label_list)\n",
    "        # results_df = scoring(report_df)\n",
    "        # results_df.to_csv(\n",
    "        #     fr\"D:\\Google Drive\\Meine Ablage\\HSLU\\Master\\Master Thesis\\Coding Area Bulldozer\\Data\\Results\\Wiki Only\\{company_name_new}_Paragraphed_Cosine_Scoring.csv\")\n",
    "        # shutil.move(\n",
    "        #     fr\"{path}\", fr\"D:\\Google Drive\\Meine Ablage\\HSLU\\Master\\Master Thesis\\Coding Area Bulldozer\\Data\\Sustainability Reports\\Already Done\\{company_name}.pdf\")\n",
    "        print(f\"{company_name_new} SAFED\")\n",
    "    except:\n",
    "        shutil.move(\n",
    "            fr\"D:\\Google Drive\\Meine Ablage\\HSLU\\Master\\Master Thesis\\Coding Area Bulldozer\\Data\\Sustainability Reports\\To Do\\{company}\", fr\"D:\\Google Drive\\Meine Ablage\\HSLU\\Master\\Master Thesis\\Coding Area Bulldozer\\Data\\Sustainability Reports\\Not Working\\{company}\")\n",
    "        print(f\"{company} didn't work\" )\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fcf02d00a6b3e7f0f947d643ae902f18bdd390f284f55fc20de5769d141d0127"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
