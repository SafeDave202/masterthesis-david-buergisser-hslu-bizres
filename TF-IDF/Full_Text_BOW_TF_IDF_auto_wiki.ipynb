{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Text Analysis with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\daveb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\daveb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import io\n",
    "import itertools\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "import contractions\n",
    "import gensim.downloader as api\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import spacy\n",
    "import torch\n",
    "import torchvision\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import TreebankWordTokenizer, sent_tokenize, word_tokenize\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from pdfminer3.converter import PDFPageAggregator, TextConverter\n",
    "from pdfminer3.layout import LAParams, LTTextBox\n",
    "from pdfminer3.pdfinterp import PDFPageInterpreter, PDFResourceManager\n",
    "from pdfminer3.pdfpage import PDFPage\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "from spacy.lang.en import English\n",
    "from transformers import pipeline\n",
    "\n",
    "sys.path.append(r\"..\")\n",
    "\n",
    "from nlp_functions import (classifier, remove_colons, remove_digits, remove_n,\n",
    "                           remove_redundant_whitespaces,\n",
    "                           remove_strange_characters, remove_stripes,\n",
    "                           text_loader)\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    \"\"\"Wendet verschiedene Standard-Preprocessings auf den Text an.\"\"\"\n",
    "\n",
    "    text = remove_strange_characters(text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "\n",
    "    text = remove_n(text)\n",
    "\n",
    "    text = remove_colons(text)\n",
    "\n",
    "    text = text.replace(r\" .\",\".\")\n",
    "    text = text.replace(r\"..\",\".\")\n",
    "    text = text.replace(r\"...\",\".\")\n",
    "\n",
    "    text = remove_stripes(text)\n",
    "\n",
    "    text = remove_redundant_whitespaces(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\daveb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\daveb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "def lemmatize_words(text):\n",
    "    \"\"\"Formt Worte im Text in ihre Lemma um\"\"\"\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = word_tokenize(text)\n",
    "\n",
    "    lemma_list = []\n",
    "\n",
    "    for word in text:\n",
    "        lemma_word = lemmatizer.lemmatize(word)\n",
    "        lemma_list.append(lemma_word)\n",
    "\n",
    "    lemma_text = ' '.join(lemma_list)\n",
    "\n",
    "\n",
    "    return lemma_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_small_tokens(prep_text):\n",
    "    \"\"\"Entfernt Tokens, welche kleiner als Vier sind.\"\"\"\n",
    "    prep_text_before = prep_text\n",
    "    prep_text_before = word_tokenize(prep_text_before)\n",
    "\n",
    "    for word in prep_text_before:\n",
    "        if len(word) <= 3:\n",
    "            prep_text_before.remove(word)\n",
    "    prep_text_list = ' '.join(prep_text_before)\n",
    "\n",
    "\n",
    "    return prep_text_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_words(corpus, n=None):\n",
    "    \"\"\"Errechnet die N-Meistgenannten Worte (BOW)\"\"\"\n",
    "    corpus=[corpus]\n",
    "\n",
    "    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    top_n_words_df = pd.DataFrame(words_freq, columns =['Word', 'Count'])\n",
    "\n",
    "    return top_n_words_df.loc[:(n-1)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_idf(text, n=None):\n",
    "    \"\"\"Errechnet den TF-IDF Score für die N-höchsten Scores\"\"\"\n",
    "    text = re.findall(r'(?:\\d[.]|[^.])*(?:[.]|$)', text)\n",
    "    tfIdfVectorizer=TfidfVectorizer(use_idf=True)\n",
    "    tfIdf = tfIdfVectorizer.fit_transform(text)\n",
    "    df = pd.DataFrame(tfIdf[0].T.todense(), index=tfIdfVectorizer.get_feature_names(), columns=[\"TF-IDF\"])\n",
    "    df = df.sort_values('TF-IDF', ascending=False)\n",
    "    return (df[:(n)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"..\\Data\\Zieltexte\\wiki_artikel_fulltext.csv\", index_col=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>sustainability</td>\n",
       "      <td>Sustainability is a societal goal that broadly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>human rights</td>\n",
       "      <td>Human rights are moral principles or norms for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>fraud</td>\n",
       "      <td>In law, fraud is intentional deception to secu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>social issues</td>\n",
       "      <td>A social issue is a problem that affects many ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>labour law</td>\n",
       "      <td>Labour laws (also known as labor laws or emplo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0           Label  \\\n",
       "0           0  sustainability   \n",
       "1           1    human rights   \n",
       "2           2           fraud   \n",
       "3           3   social issues   \n",
       "4           4      labour law   \n",
       "\n",
       "                                                Text  \n",
       "0  Sustainability is a societal goal that broadly...  \n",
       "1  Human rights are moral principles or norms for...  \n",
       "2  In law, fraud is intentional deception to secu...  \n",
       "3  A social issue is a problem that affects many ...  \n",
       "4  Labour laws (also known as labor laws or emplo...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text before cleaning:  1419734\n",
      "<class 'str'>\n",
      "Text after cleaning:  161365\n",
      "<class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\daveb\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text before cleaning:  2408783\n",
      "<class 'str'>\n",
      "Text after cleaning:  280237\n",
      "<class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\daveb\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text before cleaning:  1304877\n",
      "<class 'str'>\n",
      "Text after cleaning:  156662\n",
      "<class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\daveb\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text before cleaning:  416889\n",
      "<class 'str'>\n",
      "Text after cleaning:  48684\n",
      "<class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\daveb\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text before cleaning:  1372542\n",
      "<class 'str'>\n",
      "Text after cleaning:  160030\n",
      "<class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\daveb\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "frames = []\n",
    "topic_list = pd.Series.tolist(df[\"Label\"])\n",
    "text_list = pd.Series.tolist(df[\"Text\"])\n",
    "\n",
    "for topic, text in zip(topic_list, text_list):\n",
    "    prep_text = preprocessing(text)\n",
    "    lemma_words = lemmatize_words(prep_text)\n",
    "    text_cleaned = remove_small_tokens(lemma_words)\n",
    "    top_n_words = get_top_n_words(text_cleaned, 20)\n",
    "    top_n_words = list(top_n_words.itertuples(index=False, name=None))\n",
    "    tf_idf = get_tf_idf(text_cleaned, 20)\n",
    "    tf_idf=tf_idf.reset_index()\n",
    "    tf_idf = tf_idf.rename(columns={\"index\": \"Word\"})\n",
    "    idf_list = list(tf_idf.itertuples(index=False, name=None))\n",
    "    data = [(f\"{topic}\", idf_list, top_n_words)]\n",
    "    df1 = pd.DataFrame(data, columns = [\"Topic\", \"TF-IDF\", \"Top N Words\"])\n",
    "    frames.append(df1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sustainability', 0.29591566874854086),\n",
       " ('environmental', 0.22472979753639727),\n",
       " ('integrity', 0.21850133487134685),\n",
       " ('dimension', 0.203935251011822),\n",
       " ('planetary', 0.20071664116404878),\n",
       " ('loss', 0.1906867091204647),\n",
       " ('coexist', 0.15382737243694788),\n",
       " ('exceeding', 0.1478889550200678),\n",
       " ('most', 0.14560167679611352),\n",
       " ('over', 0.14560167679611352),\n",
       " ('time', 0.14146977279270365),\n",
       " ('safely', 0.1335808083412773),\n",
       " ('agree', 0.13114949649624702),\n",
       " ('regarded', 0.13114949649624702),\n",
       " ('accordingly', 0.12897461393960535),\n",
       " ('everyday', 0.12700719335294072),\n",
       " ('broadly', 0.1235588141144639),\n",
       " ('dominant', 0.12202905569463365),\n",
       " ('vary', 0.12060488467769495),\n",
       " ('commonly', 0.11572527843442822)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames[0][\"TF-IDF\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[            Topic                                             TF-IDF  \\\n",
       " 0  sustainability  [(sustainability, 0.29591566874854086), (envir...   \n",
       " \n",
       "                                          Top N Words  \n",
       " 0  [(sustainable, 1268), (social, 1148), (sustain...  ,\n",
       "           Topic                                             TF-IDF  \\\n",
       " 0  human rights  [(they, 0.3156911031032052), (being, 0.2869773...   \n",
       " \n",
       "                                          Top N Words  \n",
       " 0  [(right, 3660), (human, 2564), (international,...  ,\n",
       "    Topic                                             TF-IDF  \\\n",
       " 0  fraud  [(fraud, 0.38826095598392985), (perpetrator, 0...   \n",
       " \n",
       "                                          Top N Words  \n",
       " 0  [(fraud, 1708), (company, 582), (victim, 573),...  ,\n",
       "            Topic                                             TF-IDF  \\\n",
       " 0  social issues  [(issue, 0.4078577368920272), (social, 0.26852...   \n",
       " \n",
       "                                          Top N Words  \n",
       " 0  [(social, 580), (loneliness, 275), (people, 22...  ,\n",
       "         Topic                                             TF-IDF  \\\n",
       " 0  labour law  [(stipulated, 0.27239828555675233), (empire, 0...   \n",
       " \n",
       "                                          Top N Words  \n",
       " 0  [(employee, 1319), (worker, 988), (employer, 9...  ]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.concat(frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>TF-IDF</th>\n",
       "      <th>Top N Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sustainability</td>\n",
       "      <td>[(sustainability, 0.29591566874854086), (envir...</td>\n",
       "      <td>[(sustainable, 1268), (social, 1148), (sustain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>human rights</td>\n",
       "      <td>[(they, 0.3156911031032052), (being, 0.2869773...</td>\n",
       "      <td>[(right, 3660), (human, 2564), (international,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fraud</td>\n",
       "      <td>[(fraud, 0.38826095598392985), (perpetrator, 0...</td>\n",
       "      <td>[(fraud, 1708), (company, 582), (victim, 573),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>social issues</td>\n",
       "      <td>[(issue, 0.4078577368920272), (social, 0.26852...</td>\n",
       "      <td>[(social, 580), (loneliness, 275), (people, 22...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>labour law</td>\n",
       "      <td>[(stipulated, 0.27239828555675233), (empire, 0...</td>\n",
       "      <td>[(employee, 1319), (worker, 988), (employer, 9...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Topic                                             TF-IDF  \\\n",
       "0  sustainability  [(sustainability, 0.29591566874854086), (envir...   \n",
       "0    human rights  [(they, 0.3156911031032052), (being, 0.2869773...   \n",
       "0           fraud  [(fraud, 0.38826095598392985), (perpetrator, 0...   \n",
       "0   social issues  [(issue, 0.4078577368920272), (social, 0.26852...   \n",
       "0      labour law  [(stipulated, 0.27239828555675233), (empire, 0...   \n",
       "\n",
       "                                         Top N Words  \n",
       "0  [(sustainable, 1268), (social, 1148), (sustain...  \n",
       "0  [(right, 3660), (human, 2564), (international,...  \n",
       "0  [(fraud, 1708), (company, 582), (victim, 573),...  \n",
       "0  [(social, 580), (loneliness, 275), (people, 22...  \n",
       "0  [(employee, 1319), (worker, 988), (employer, 9...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(fr\"..\\Data\\Resultate\\Testfolder\\TF-IDF Wiki\\wiki_bow_tf_idf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1 (tags/v3.10.1:2cd268a, Dec  6 2021, 19:10:37) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5cf922369b83184eb6b2ab0a3b64d08061786420dbf1332a02b89d0aac9562ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
