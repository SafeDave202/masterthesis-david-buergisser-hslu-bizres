{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero Shot Learning Fulltext\n",
    "\n",
    "In diesem Script werden die Scores für den Zero-Shot-Learning Fulltext Ansatz errechnet."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zuerst werden die nötigen Libraries installiert und importiert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\daveb\\anaconda3\\lib\\site-packages)\n",
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'r..\\\\requirements.txt'\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\daveb\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\daveb\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\daveb\\anaconda3\\lib\\site-packages)\n",
      "WARNING: You are using pip version 21.3.1; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\daveb\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\daveb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\daveb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\daveb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\daveb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import io\n",
    "import itertools\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import unicodedata\n",
    "import sys\n",
    "\n",
    "\n",
    "import contractions\n",
    "import gensim.downloader as api\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import spacy\n",
    "import torch\n",
    "import torchvision\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import TreebankWordTokenizer, sent_tokenize, word_tokenize\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from pdfminer3.converter import PDFPageAggregator, TextConverter\n",
    "from pdfminer3.layout import LAParams, LTTextBox\n",
    "from pdfminer3.pdfinterp import PDFPageInterpreter, PDFResourceManager\n",
    "from pdfminer3.pdfpage import PDFPage\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "from spacy.lang.en import English\n",
    "from transformers import pipeline\n",
    "\n",
    "sys.path.append(r\"..\")\n",
    "\n",
    "from nlp_functions import (classifier, remove_colons, remove_digits, remove_n,\n",
    "                           remove_redundant_whitespaces,\n",
    "                           remove_strange_characters, remove_stripes,\n",
    "                           text_loader)\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funktionen\n",
    "\n",
    "Im Folgenden werden notwendige Funktionen definiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_loader(company_name): \n",
    "    \"\"\"Nimmt PDF-File Namen entgegen. Gibt Text als String, PDF-Namen, Pfad inkl. PDF-Namen sowie Pfad ohne PDF-Namen zurück\"\"\"\n",
    "    company_name = company_name[:-4]\n",
    "    source = r'..\\Data\\Nachhaltigkeitsberichte\\Alle'\n",
    "    path = rf\"{source}\\{company_name}.pdf\"\n",
    "\n",
    "    text = text_loader(path)   \n",
    "\n",
    "    return text, company_name, path, source \n",
    "\n",
    "def topic_modeler(text):\n",
    "    \"\"\"Errechnet die Klassifizierungs-Wahrscheinlichkeiten für den Text für die vorgegebenen Label aus\"\"\"\n",
    "\n",
    "    classifier_pipeline = pipeline(\n",
    "        \"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "    input_sequence = text\n",
    "    label_candidate = ['sustainability', 'human rights',\n",
    "                       'fraud', 'social issues', 'labour law']\n",
    "    x = classifier_pipeline(input_sequence, label_candidate)\n",
    "    result_label = x[\"labels\"]\n",
    "    result_score = x[\"scores\"]\n",
    "    tuple_for_df = list(zip(result_label, result_score))\n",
    "    df_topic_modeling_score = pd.DataFrame(\n",
    "        tuple_for_df, columns=[\"Label\", \"Score\"])\n",
    "    return df_topic_modeling_score\n",
    "\n",
    "def preprocessing_text(text):\n",
    "    \"\"\"Wendet verschiedene Standard-Preprocessings auf den Text an.\"\"\"\n",
    "\n",
    "\n",
    "    text = remove_strange_characters(text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "\n",
    "    text = remove_n(text)\n",
    "\n",
    "    text = remove_colons(text)\n",
    "\n",
    "    text = text.replace(r\" .\",\".\")\n",
    "    text = text.replace(r\"..\",\".\")\n",
    "    text = text.replace(r\"...\",\".\")\n",
    "\n",
    "    text = remove_stripes(text)\n",
    "\n",
    "    text = remove_redundant_whitespaces(text)\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TI-IDF Funktionen\n",
    "\n",
    "Dies sind Funktionen, welche spezifisch für die Bennennung der ersten Spalte benötigt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_words(text):\n",
    "    \"\"\"Formt Worte im Text in ihre Lemma um\"\"\"\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = word_tokenize(text)\n",
    "\n",
    "    lemma_list = []\n",
    "\n",
    "    for word in text:\n",
    "        lemma_word = lemmatizer.lemmatize(word)\n",
    "        lemma_list.append(lemma_word)\n",
    "\n",
    "    lemma_text = ' '.join(lemma_list)\n",
    "\n",
    "\n",
    "    return lemma_text\n",
    "\n",
    "\n",
    "\n",
    "def remove_small_tokens(prep_text):\n",
    "    \"\"\"Entfernt Tokens, welche kleiner als Vier sind.\"\"\"\n",
    "    prep_text_before = prep_text\n",
    "    prep_text_before = word_tokenize(prep_text_before)\n",
    "\n",
    "    for word in prep_text_before:\n",
    "        if len(word) <= 3:\n",
    "            prep_text_before.remove(word)\n",
    "    prep_text_list = ' '.join(prep_text_before)\n",
    "\n",
    "\n",
    "    return prep_text_list\n",
    "\n",
    "def get_top_n_words(corpus, n=None):\n",
    "    \"\"\"Errechnet die N-Meistgenannten Worte (BOW)\"\"\"\n",
    "    corpus=[corpus]\n",
    "\n",
    "    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    top_n_words_df = pd.DataFrame(words_freq, columns =['Word', 'Count'])\n",
    "\n",
    "    return top_n_words_df.loc[:(n-1)]\n",
    "\n",
    "\n",
    "def get_tf_idf(text, n=None):\n",
    "    \"\"\"Errechnet den TF-IDF Score für die N-höchsten Scores\"\"\"\n",
    "    text = re.findall(r'(?:\\d[.]|[^.])*(?:[.]|$)', text)\n",
    "    tfIdfVectorizer=TfidfVectorizer(use_idf=True)\n",
    "    tfIdf = tfIdfVectorizer.fit_transform(text)\n",
    "    df = pd.DataFrame(tfIdf[0].T.todense(), index=tfIdfVectorizer.get_feature_names(), columns=[\"TF-IDF\"])\n",
    "    df = df.sort_values('TF-IDF', ascending=False)\n",
    "    return (df[:(n)])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eingabetexte\n",
    "\n",
    "Dies sind die Texte, welche bearbeitet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_companies = os.listdir(r\"..\\Data\\Nachhaltigkeitsberichte\\Alle\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Durchführungs-Loop\n",
    "\n",
    "In einem Loop werden nun alle Berichte einer nach dem anderen durchgegangen. Die Ausgabewerte werden im entsprechenden Ordner abgelegt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We process now 4f303cec-a12d-480b-accb-7b56f706f60e_axa-ri2020-en-accessible.pdf\n",
      "axa_health_customer SAFED\n",
      "We process now 4f391131-ad12-ab53-7265-5e6c88840627.pdf\n",
      "global_safety_training SAFED\n",
      "We process now 5ZmOsI2P3oe0plCvOThrCySgcDcKCXqj.pdf\n",
      "5ZmOsI2P3oe0plCvOThrCySgcDcKCXqj.pdf didn't work\n",
      "We process now 9clin.pdf\n"
     ]
    }
   ],
   "source": [
    "for company in list_of_companies:\n",
    "    try:\n",
    "        print(f\"We process now {company}\")\n",
    "        \n",
    "        ## BOW / TF-IDF\n",
    "\n",
    "        text_1, company_name, path, source = pdf_loader(company) #\n",
    "        prep_text = preprocessing_text(text_1)\n",
    "        lemma_words = lemmatize_words(prep_text)\n",
    "        text_cleaned = remove_small_tokens(lemma_words)\n",
    "        top_n_words = get_top_n_words(text_cleaned, 20)\n",
    "        top_n_words = list(top_n_words.itertuples(index=False, name=None))\n",
    "        tf_idf = get_tf_idf(text_cleaned, 20)\n",
    "        tf_idf = tf_idf.reset_index()\n",
    "        tf_idf = tf_idf.rename(columns={\"index\": \"Word\"})\n",
    "        idf_list = list(tf_idf.itertuples(index=False, name=None))\n",
    "        data = [(company_name, idf_list, top_n_words)]\n",
    "        df1 = pd.DataFrame(data, columns = [\"Company Name\", \"TF-IDF\", \"Top N Words\"])\n",
    "        company_name_new = f\"{df1['Top N Words'][0][0][0]}_{df1['Top N Words'][0][1][0]}_{df1['Top N Words'][0][2][0]}\"\n",
    "\n",
    "        ## Full Text Topic Modeling\n",
    "\n",
    "        text = pdf_loader(company)\n",
    "        text = preprocessing_text(text)\n",
    "        result_df = topic_modeler(text) # hier werden die Scores für den Text vergeben\n",
    "        result_df.to_csv(\n",
    "            fr\"..\\Data\\Resultate\\Zero Shot Learning Fulltext\\{company_name_new}_FullText_TopicModeling.csv\")\n",
    "\n",
    "        print(f\"{company_name_new} SAFED\")\n",
    "    except:\n",
    "        print(f\"{company} didn't work\" )\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fcf02d00a6b3e7f0f947d643ae902f18bdd390f284f55fc20de5769d141d0127"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
